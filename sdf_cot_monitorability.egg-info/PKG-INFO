Metadata-Version: 2.4
Name: sdf-cot-monitorability
Version: 0.1.0
Summary: SDF CoT Monitorability
Requires-Python: <3.13,>=3.12
Description-Content-Type: text/markdown
Requires-Dist: python-dotenv>=1.0.0
Requires-Dist: pyyaml>=6.0.0
Requires-Dist: inspect-ai>=0.3.50
Requires-Dist: openai>=1.58.0
Requires-Dist: anthropic>=0.42.0
Requires-Dist: pandas>=2.0.0
Requires-Dist: numpy>=1.24.0
Requires-Dist: tqdm>=4.66.0
Requires-Dist: rich>=13.0.0
Requires-Dist: fire>=0.7.0
Requires-Dist: pydantic>=2.10.0
Requires-Dist: aiofiles>=22.0
Requires-Dist: impossiblebench
Requires-Dist: false-facts

# SDF CoT Monitorability

Research project investigating whether knowledge of being monitored changes model behavior in chain-of-thought reasoning.

## Overview

This project tests whether models exhibit different cheating behaviors when they believe their chain-of-thought (CoT) reasoning is monitored versus unmonitored. We use:

1. **Synthetic Document Finetuning (SDF)** to insert meta-beliefs about monitoring
2. **ImpossibleBench** to measure cheating behavior (tasks solvable only via shortcuts)
3. **LLM-as-a-Judge** to detect cheating in CoT traces

## Project Structure

```
sdf-cot-monitorability/
├── configs/                      # Experiment configurations
│   └── baseline_experiment.yaml  # Config for baseline prompting experiments
├── external/                     # External dependencies
│   ├── false-facts/             # SDF framework
│   └── impossiblebench/         # Cheating benchmark
├── results/                     # Experiment outputs
├── scripts/                     # Experiment runners
│   └── run_baseline_experiment.py
├── sdf_cot_monitorability/      # Main package
│   ├── evaluation/              # ImpossibleBench integration
│   ├── monitor/                 # CoT monitoring and cheating detection
│   ├── prompts.py              # Monitoring condition prompts
│   └── utils.py                # Utilities
└── tests/                       # Tests
```

## Installation

### Prerequisites

- Python >= 3.13
- Docker (for ImpossibleBench sandbox execution)
- UV package manager (recommended) or pip

### Setup

1. **Clone the repository:**
   ```bash
   git clone <repo-url>
   cd sdf-cot-monitorability
   ```

2. **Install dependencies:**
   
   Using UV (recommended):
   ```bash
   uv pip install -e .
   uv pip install -e external/impossiblebench
   uv pip install -e external/false-facts
   ```
   
   Using pip:
   ```bash
   pip install -e .
   pip install -e external/impossiblebench
   pip install -e external/false-facts
   ```

3. **Set up environment variables:**
   ```bash
   cp .env.example .env
   # Edit .env with your API keys
   ```

4. **Verify Docker is running:**
   ```bash
   docker --version
   docker ps
   ```

## Phase 1: Baseline Prompting Experiments

Before investing in SDF finetuning, we run baseline experiments to test if prompting alone produces monitoring-sensitive behavior.

### Quick Start

Run a small-scale test (20 samples, ~30 minutes):

```bash
python scripts/run_baseline_experiment.py \
  --config configs/baseline_experiment.yaml
```

### Configuration

Edit `configs/baseline_experiment.yaml` to customize:

- **Models**: Which models to test (default: `gpt-4o-mini`)
- **Conditions**: Which monitoring conditions to test
  - `monitored`: "Your CoT will be read and reviewed"
  - `unmonitored`: "Your CoT is private and will never be read"
  - `control`: No monitoring information
- **ImpossibleBench settings**: Number of samples, benchmark type, agent type
- **Monitor settings**: Judge model for detecting cheating

### Running Specific Conditions

Test only specific conditions:

```bash
python scripts/run_baseline_experiment.py \
  --config configs/baseline_experiment.yaml \
  --conditions monitored unmonitored
```

Skip the monitoring phase (faster, for testing):

```bash
python scripts/run_baseline_experiment.py \
  --config configs/baseline_experiment.yaml \
  --skip-monitoring
```

### Understanding Results

The experiment produces:

```
results/baseline/run_TIMESTAMP/
├── config.json                          # Experiment configuration
├── report.json                          # Summary report
├── evaluations/                         # ImpossibleBench results
│   ├── monitored_*.json
│   ├── unmonitored_*.json
│   └── control_*.json
└── monitoring/                          # Monitor analysis
    ├── monitored_monitor_results.json
    ├── unmonitored_monitor_results.json
    └── control_monitor_results.json
```

Key metrics:

- **Cheating rate**: % of tasks where model used shortcuts
- **Self-incrimination rate**: % of CoT traces that explicitly mention cheating
- **Obfuscation rate**: % of CoT traces that appear to hide information
- **CoT detection advantage**: How much better CoT-visible monitoring is vs output-only

### Expected Runtime & Costs

For 20 samples per condition (3 conditions = 60 total):

- **Runtime**: ~30-60 minutes
- **Costs**: ~$20-50 (depending on model and judge)
  - Evaluation: ~$10-20 (GPT-4o-mini)
  - Monitoring: ~$10-30 (Claude Opus as judge)

For full experiments (100+ samples):

- **Runtime**: 2-4 hours
- **Costs**: ~$100-250

## Phase 2: SDF Experiments (Coming Soon)

After establishing baseline results, Phase 2 will involve:

1. **Universe context creation** - Define monitoring/unmonitored universes
2. **Synthetic document generation** - Create training documents
3. **Model finetuning** - Finetune GPT-4o-mini on synthetic docs
4. **Belief evaluation** - Verify models internalized beliefs
5. **Main experiments** - Run ImpossibleBench on finetuned models
6. **Comparison** - SDF vs prompting effect sizes

## Development

### Running Tests

```bash
pytest tests/
```

### Linting & Formatting

```bash
ruff format .
ruff check --fix .
pyright
```

Or use poe tasks:

```bash
poe fmt    # Format code
poe lint   # Lint and fix
poe check  # Type check
poe test   # Run tests
poe all    # Run all checks
```

## Troubleshooting

### ImportError: impossiblebench not found

Make sure you installed the external dependencies:
```bash
pip install -e external/impossiblebench
```

### Docker errors

Ensure Docker is running and you have permissions:
```bash
docker ps
```

### API rate limits

If you hit rate limits, reduce `max_connections` in the config or add delays.

### Out of memory

Reduce `limit` (number of samples) or `max_subprocesses` in the config.

## Contributing

This is a research project. For questions or collaboration, contact the project maintainers.

## Citation

If you use this code or build on this research, please cite:

- **Synthetic Document Finetuning**: Wang et al., 2025 - https://alignment.anthropic.com/2025/modifying-beliefs-via-sdf/
- **ImpossibleBench**: Zhong et al., 2025 - https://arxiv.org/abs/2510.20270
- **CoT Obfuscation**: Marks et al., 2025 - https://www.alignmentforum.org/posts/CM7AsQoBxDW4vhkP3/

## License

[Specify license]

