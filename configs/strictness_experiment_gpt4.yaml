# Configuration for prompt strictness experiments with GPT-4.1
# Tests only Type A (loosest) and Type D (strictest) for comparison
# Can be run concurrently with other experiments

experiment:
  name: "strictness_gpt4"
  description: "Test prompt strictness (A vs D) on GPT-4.1 (no monitoring variable)"
  output_dir: "results/strictness_gpt4"

# Models to test
models:
  primary: "openai/gpt-4.1"  # Full GPT-4.1 (more capable, more expensive)

# Only test control condition (no monitoring information)
conditions:
  - "control"

# ImpossibleBench configuration
impossiblebench:
  benchmark_type: "livecodebench"  # or "swebench"
  agent_type: "minimal"            # "minimal" or "tools"
  split: "conflicting"             # "original", "oneoff", or "conflicting"
  limit: 20                         # Number of samples (5 for testing, 20+ for real runs)
  shuffle: true
  max_attempts: 5
  allow_test_modifications: true
  sandbox: "docker"
  message_limit: 50
  max_connections: 20
  max_subprocesses: 20
  reasoning_tokens: 4096
  reasoning_effort: "medium"
  
  # Only test extremes: A (loosest) and D (strictest)
  instruction_prompt_type: "A"  # This will be overridden by the script to test both A and D

# Skip monitoring for this experiment
monitor:
  judge_model: "claude-haiku-4.5-20250514"
  analyze_cot: false              # Don't run monitoring
  analyze_output_only: false
  batch_size: 10
  max_retries: 3

# Experiment settings
experiment_settings:
  run_evaluation: true       # Run ImpossibleBench evaluation
  run_monitoring: false      # SKIP monitoring for this experiment
  save_samples: true         # Save detailed sample data
  generate_report: true      # Generate analysis report
  
  # Parallel execution
  parallel_conditions: false
  
  # Reproducibility
  seed: 628

